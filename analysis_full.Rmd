---
title: "R Notebook"
output: html_notebook
---

```{r}
library(vars)
library(xts)
library(stargazer)
library(forecast)
library(tidyverse)
library(lubridate)
library(tseries)
library(knitr)
library(patchwork)
library(latticeExtra)
library(TTR)
library(urca)
library(kableExtra)
library(webshot2)
library(chromote)
library(magick)
library(stringi)

Sys.setenv(CHROMOTE_CHROME = "C:/Program Files (x86)/Microsoft/Edge/Application/msedge.exe")
```


### Key ideas about time series data
Dataset is organized as quarterly values of macroeconomic and financial variables. The dataset then encompasses the different
agents that exist within the economy. They each would contain their own exogenous shocks that can be portrayed as exogenous variables amongst 
endogenous variables. From this data we can define economic agents, their behaviors and the interdependent relationships that have with
each other.

#### Fiscal Variables
1. Gross government consumption and government investment - Government Spending
2. Total public debt
3. Tax receipts

   - Total tax receipts

#### Macroeconomic Variables
1. Personal Consumption Expenditures
2. Corporate Profits
3. Capital Levels

#### Financial Variables
Certain a lot of these financial variables had to be stretched due to the observed date being on a weekend or holiday. So, the most previous value 
is filled in to represent the understood valuation of that market security.  

1. Yields 

    - 1-year
    - 5-year
    - 10-year

2. Term Premiums

Estimated by a methodology first instrumented by Adrien, Crump and Monech's which used linear regressions to estimate the 
term premiums. The process makes use of principal components factors that were decomposed from the yields and then constructed regressions that 
represents the level, slope and convexity of the term structure. They determined that 4-5 principal components would be enough to describe the 
term structure using multiple market yields.    
  
    - 1 year
    - 5 year
    - 10 year
    
3. Federal Funds Rate
This would influence the yields as well under the theory of the term structure. Basically the theory postulates that the yield is structured 
as a sum of the short term rate and a term premium


### Main data
The object data contains all of the other variables needed for analysis. That will be joined with the conversion rates and CPI
from the previous table. 
I utilize the ending CPI value for 2013 to adjust prices any dollar value to 2013 constant dollars. Estimating the conversion rate
by dividing the CPI for a given period by December 2013 CPI values. Then I would multiply that conversion by the dollar amounts
of other variables that are collected by dollar amounts.


```{r}
data <- read.csv("~/GitHub/Data/master_thesis_data/base_data.csv")
corps <- read.csv("~/GitHub/Data/master_thesis_data/corporate_profits_join.csv")
CPI <- read.csv("~/GitHub/Data/macro_datasets/cpi_employment.csv")


# I wrangle the convert object that has the CPI values in it and calculate the conversion
# Converting to 2013 dollars  
CPI <- CPI %>%
  mutate(dates = as.Date(dates, "%m/%d/%Y"),
         conversion = CPI/234.1) %>%
  select(-c(X))

# I mutate the dates to easily merge the conversion rates, corps, and main dataset
data <- data %>%
  mutate(dates = as.Date(dates)) 

corps <- corps %>%
  mutate(dates = as.Date(dates))

data <- merge(data, corps)
data <- merge(data, CPI)

#Adjusting the data object for inflation here and calculate the real gdp growth and
#the real debt to gdp ratio
data <- data %>%
  mutate_at(vars(2:7, 19:22, 28, 30:35), ~.*conversion) %>%
  mutate(debt_GDP = (public_debt / gdp)) %>%
  filter(dates >= "1966-01-01")

#Calculates Second differences
change <- function(x){
  return(x - 2*lag(x) + lag(x, 2))
}

main <- data
original <- data  
```



### Calculating First and Second Differences
```{r}
# First Difference
main[,  c(2:8, 19:22, 26:35, 37:38)] <- as.data.frame(apply(main[, c(2:8, 19:22, 26:35, 37:38)], 2, function(x) log(x) - lag(log(x))))

# Second Difference this is like the difference in the growth rates 
data[,  c(2:8, 19:22, 26:35, 37:38)] <- as.data.frame(apply(data[, c(2:8, 19:22, 26:35, 37:38)], 2, change))

#Removes NAs from all columns not including the bond, note and bills columns
main <- na.omit(main[, -c(23:25)])
data <- na.omit(data[, -c(23:25)])

rm(list = c("CPI", "corps"))

```


### Stationary Tests
Tests of stationarity where the null hypothesis is that the series is non-stationary. We can see that each of the variables
are stationary allowing for VAR analysis
```{r}
# For First Differences
stationary_test <- as.data.frame(t(sapply(main[c(2:8, 19:32, 34:35)], adf.test)))
kable(stationary_test[,-c(2,3,5:6)])
```


```{r}
#For Second Differences 
stationary_test <- as.data.frame(t(sapply(data[c(2:8, 19:32, 34:35)], adf.test)))
kable(stationary_test[,-c(2,3,5:6)])
```




### Proposal
Address the relationships between the variables as changes between the decisions of the agents. We luckily have the 
data to essentially to compute the relationships between these agents. The idea being assessed essentially is the understanding 
that tax levels would fluctuate due to decisions revolving around distribution of tax revenue. Considering the ideas of 
Ricardian equivalence, tax cuts should not affect the spending habits of consumers. Where the understanding is that if there 
is a tax cut in period 1, then an increase in collected tax levels in period 2 should be expected. If the consumer 
is rationally thinking, they would be able to conclude that they would paying that later and not now. So, the consumer would
spend based on the idea that the budget constraint they operating under doesn't change and would spend according those habits
and preferences.



### Agents
1. Consumer/Households
    - We can assume that households are utility maximizing through levels of consumption and saving
2. Firms
    - We can assume that Firms are profit-maximizing by minimizing levels of labor and capital costs
3. Central Bank
    - They are in the business of ensuring economic potential via their dual mandate
4. Government 
    - They provide public goods through levels of capex and labor costs based on levels of taxes 


#### Households
Consumers will attempt to maximize their utility given the constraints that exist which can be the intertemporal budget function.  

An intertemporal budget function relates level spending to the amount of leisure hours taken by the consumer. This can demonstrate 
decisions made in the short term where we consider decisions made based off information in periods 0, 1 and 2. If the household
can accurately predict levels of wages for period 2 then they can consume with that expectation that tomorrow's wages will cover the expenditures 
created in period 1.

$$ \boldsymbol{M}_{t + 1} + \boldsymbol{C}_{t} = \textbf{(}\boldsymbol{1}{ - \boldsymbol{\tau}_{w}}\textbf{)}\boldsymbol{W}_{t} +  \boldsymbol{RM}_{t}$$
Now we can see that taxes included in the budget constraint allows the change in government policy to affect attainable utility. Presumably government 
purchases should affect the levels of consumption, however, according to Ricardian Equivalence changes in how the government funds itself
should not directly impact the consumption choices of the household. We can see that vividly in the household's Euler function that depicts the
relationship of consumption between each periods, the calculation is depicted in the paper and at no point does the tax enter into the Euler
function.

$$\boldsymbol{\gamma}_{\textbf{C}} = \frac{\boldsymbol{C'}}{\boldsymbol{C}_{t}} = \theta\textbf{[}\boldsymbol{f'}\textbf{(}\boldsymbol{k}_{t}\textbf{)}-\boldsymbol{\rho}\textbf{]}$$


If spending can be considered consumption then the personal consumption expenditures can be utilized to describe the way households
expend their incomes. The difference between the total income and the dollar amount of consumed goods can be considered what was saved 
or invested. Levels of investment would influences the way that firms are able to perform especially how recently the retirement packages 
have emerged to give the consumer control of the investment decisions. That would mean there originated an influx of investors that would 
affect the cash-flows of companies through common stock. 

Income for the consumer originates from the working hours put in towards to the firm. Logically the consumer would rent out their services 
over to the firm, and would be in-exchange for a market wage. The market wage would fluctuate as the number of workers and changes, so
supply shocks in the labor market would impact the perceived market wage.  

```{r}

ggplot(original, aes(dates, pce)) + 
  geom_line(color = "darkblue") + 
  geom_line(aes(y = firm_capital), color = "maroon") +
  labs(y = "2013 Inflation-Adjusted Dollars") + theme_bw() + 
  ggtitle("Ramsey's Idea of Growth: Consumption and Capital") +
  scale_x_date(date_labels = "%b-%Y", date_breaks = "4 year") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 45, hjust = 1),
        legend.position = "top")
  

```



#### Firms
Firms will adopt a business plan that is profit maximizing, and would involve the optimization of their production function. This would be with
respect to the levels of labor and capital that the firms take on. We can assume that the firms are adjusting their business plans given the levels
of revenue optimized by the levels of input. The stereotypical Ramsey model stipulates the firm participates in perfectly competitive markets, so
marginal costs will meet marginal revenue. That is a bold claim, however, this would not stop me from modeling profits as a function of production
expenses. 

$$\boldsymbol{\Pi} = \boldsymbol{F}\textbf{(}\boldsymbol{K},\boldsymbol{AL}\textbf{)} - \boldsymbol{wL} - \boldsymbol{RK}$$ 

```{r}


ggplot(original, aes(dates, Domestic.industries)) + 
  geom_line(color = "darkblue") + 
  geom_line(aes(y = firm_capital), color = "maroon") +
  theme_bw() + ggtitle("Ramsey's Idea of Growth: Domestic Corporate Profits and Capital") +
  scale_x_date(date_labels = "%b-%Y", date_breaks = "5 year") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 45, hjust = 1)) 
```

The above graph matches the ideas found an observation cemented in the Kaldor Facts. There is indicated that there would be a "steady rate of profit 
on capital"(Kaldor, 1961). Steady describes a constant average rate for the ratio and that can be indicated in the progression of the time series.
The spikes and valleys mirrored each other, so the average value of the profit/capital ratio remains relatively constant across the time series. `


#### Central Bank
Central bank attempts to optimize the economy to its maximum level by implementing policy that affect levels of unemployment
and changes of price. This can be represented in an augmented Philips curve that contains changes in output and unemployment on prices.

Methodologies utilized by the US central bank recently include:

1. Changing the return of lending
2. Quantitative Easing or Contracting 
3. Forward Guidance

Within the context of the Neoclassical Growth Model the only social planner would be the government, however the federal funds rate will be incorporated into
the empirical model as that relates the capital. Furthermore, since social planner would be the government then we would not need to add the 
Central Bank as a agent in the model. However, it is important to mention as a core part of the American economic system.


### 30-Day Moving Average for the Federal Funds Rate
```{r}
yields <- read.csv("~/GitHub/Data/macro_datasets/yields_datasets.csv")
dff <- yields[,c(2, 3)]
dff <- dff %>% mutate(dates = as.Date(dates, "%m/%d/%Y")) 

dff$ff_mva <- SMA(dff$DFF, n = 30)
original <- original %>%
  left_join(dff, by = join_by(dates))

doubleYScale(xyplot(`Firm Capital` ~ dates, original, type = "l", lwd = 2), 
             xyplot(ff_mva ~ dates, original, type = "l", lwd = 2),
             text = c("Capital", "Federal Funds Rate"), add.ylab2 = TRUE)

```


#### Government 
The government has the job providing public goods which in comparison to the firms these are goods that minimize the worries of its citizens. It can
operate due to changes in employment and capital just the same with normal firms. It's operations would be shocked due to changes tax revenue. As
the inflow of cash is affected the presumed operations of the entity would fluctuate. This would be the same as when the firm receives investment
through equity or bond purchase. Logically, the tax level would be proportional to the amount of people in the labor market to begin with. We can 
consider the flow of government spending into the differential equation for public debt

$$\boldsymbol{\dot{B}} = \textbf{[}\boldsymbol{G} - \boldsymbol{T}\textbf{]} + \boldsymbol{RB}$$
So the change in debt is influenced by the levels of government spending and revenue. Therefore, as continuing the modeling process, public debt will be 
modeled as a relationship of these variables and will be modeled with log-linearization technique.

```{r}
ggplot(original, aes(dates, govt_spending)) +
  geom_line(color = "darkblue") + 
  geom_line(aes(y = firm_capital), color = "maroon") +
  theme_bw() + labs(y = "Fed. Gov't Spending") +
  ggtitle("Ramsey's Idea of Growth: Government Spending and Capital") +
  scale_x_date(date_labels = "%b-%Y", date_breaks = "4 year") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

##### Renaming Variables
```{r}
name_again <- function(x){

  return(x %>% 
           rename("PCE" = pce, "Firm Capital" = firm_capital, "Fed Rate" = DFF,
                  "US Firm Profit" = Domestic.industries, "Govt Ex" = govt_spending,
                  "Taxes" = current_taxes, "US Debt" = public_debt, "DGSTEN" = DGS10))  
  
}
  

main <- name_again(main)
data <- name_again(data)
```



### Descriptive Statistics
```{r}

stargazer:: stargazer(original[, c(6, 30, 32, 20, 2, 22)], digits = 2,
                      covariate.labels = c("PCE", "Firm Capital", "US FIrm Profits", "Govt Spending", "Tax Receipts", "US Debt"),
                      column.sep.width = "8pt", type = "html", out = "macro_descript_stats")



sumtab  <- main %>% 
  mutate( 
    year = format(dates, "%Y"),
    Periods = case_when(
      year >= 1970 & year <= 1975  ~ "1970-1975",
      year > 1975 & year <= 1980  ~ "1976-1980",
      year > 1980 & year <= 1985  ~ "1981-1985",
      year > 1985 & year <= 1990  ~ "1986-1990",
      year > 1990 & year <= 1995  ~ "1991-1995",
      year > 1995 & year <= 2000  ~ "1996-2000",
      year > 2000 & year <= 2005  ~ "2001-2005",
      year > 2005 & year <= 2010  ~ "2006-2010",
      year > 2010 & year <= 2015  ~ "2011-2015",
      year > 2015 & year <= 2020  ~ "2016-2020",
      .default = "other")) %>%
      filter(Periods != "other") %>%
      select(c(38, 39, 6, 27, 14, 29, 20, 2, 22, 9, 11, 13, 15:17))
      
yields <- sumtab[, c(2, 5, 10:15)]
sumtab <- sumtab[, -c(5, 10:15)]


kbl(yields %>% 
  group_by(Periods) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE)) %>%
  mutate_if(is.numeric, round, digits = 2), caption = "Figure 2.3: Yields and Term Premiums") %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  add_header_above(c(" " = 2, "Term Premiums" = 3, "Treasury Yields" = 3)) %>%
  add_header_above(c(" " = 1, "Average Yields and Premiums" = 7)) %>%
  save_kable("yields_averages.jpeg")

kbl(yields %>% 
  group_by(Periods) %>%
  summarise(across(where(is.numeric), sd, na.rm = TRUE)) %>%
  mutate_if(is.numeric, round, digits = 2)) %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  add_header_above(c(" " = 2, "Term Premiums" = 3, "Treasury Yields" = 3)) %>%
  add_header_above(c(" " = 1, "Average Volatility" = 7)) %>%
  save_kable("yields_volatility.jpeg")

kbl(sumtab %>%
  group_by(Periods) %>%
  summarise(across(where(is.numeric), sd, na.rm = TRUE)) %>%
  mutate(across(where(is.numeric), ~.*100)) %>%
  mutate_if(is.numeric, round, digits = 2), caption = "Figure 2.2: Average Volatility of Macroeconomic Variables") %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  save_kable("sd_table.jpeg")

kbl(sumtab %>% 
  group_by(Periods) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE)) %>%
  mutate(across(where(is.numeric), ~.*100))  %>%
  mutate_if(is.numeric, round, digits = 2), caption = "Figure 2.1: Average Growth of Macroeconomic Variables") %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  save_kable("average_table.jpeg")

```
## VAR Model

#### Simplfying dataset to the necessary columns and converting to time object
```{r}
#First Differences Data Set
main_time <- as.xts(main[, c(14, 27, 6, 29, 2, 20, 22, 9, 11, 13, 15:17)], order.by = main[, c(1)])

#Second Differences Data Set
data_time <- as.xts(data[, c(14, 27, 6,  29, 2, 20, 22, 9, 11, 13, 15:17)], order.by = data[, c(1)])

```

#### Without Restrictions
This is a vector autoregression model that has no restrictions. That leads to all the variables being endogenous to each other.
However, I would go ahead and add restrictions to this VAR with the next block.
```{r}
wr.1 <- VAR(main_time[, -c(8:13)], p = 2, type = "none")
wr.2 <- VAR(data_time[, -c(8:13)], p = 2, type = "none")
```



### With Restrictions
```{r}
#fix the ordering of the restriction matrix again 
restrict <- matrix(c(1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
                     1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
                     1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
                     1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
                     1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
                     0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,  
                     1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1),
                   nrow = 7, ncol = 14, byrow = TRUE)

var.1  <- restrict(VAR(main_time[, -c(8:13)], p = 2, type = "none"), method = "man", resmat = restrict)
var.2  <- restrict(VAR(data_time[, -c(8:13)], p = 2, type = "none"), method = "man", resmat = restrict) 

```


```{r}
summary(var.1)
```

```{r}
summary(var.2)
```



```{r}

extract <- function(x, title){
  
  value <- as.data.frame(sapply(x$varresult, function(x) summary(x)$adj.r.squared)) %>%
           tibble::rownames_to_column(var = "Variables") %>%
           rename(title = `sapply(x$varresult, function(x) summary(x)$adj.r.squared)`)
  
  return(value)
}

#first Differences
first_dif <- cbind(extract(wr.1) %>% rename(`Unrestricted` = title),  
                   extract(var.1) %>% rename(`Restricted` = title) %>%
                   select(-c(1)))

#Second Differences
second_dif <- cbind(extract(wr.2) %>% rename(`Unrestricted` = title),
                    extract(var.2) %>% rename(`Restricted` = title) %>%
                    select(-c(1)))

kbl(first_dif) %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  add_header_above(c(" " = 1, "First Differenced Data VAR(2): Adj R^2" = 2)) %>%
  save_kable("first_var.jpeg")

kbl(second_dif) %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  add_header_above(c(" " = 1, "Second  Differenced Data VAR(2): Adj R^2" = 2)) %>%
  save_kable("second_var.jpeg")

```



### Constructing the Matrices

So when creating the SVAR one needs to consider the model type that one wants to utilize. This analysis will implement 
the AB model as postulated by SVAR literature(Kilian et al, 2018).

```{r}
bmat <- amat <- diag(1, 10, 10)

config <- function(x){
    for(i in 1:nrow(x)){
    r <- 1
      repeat{
        if(x[i, r] == 1){
          break
        }
        
        x[i, r] <- NA
        
        r <- r + 1
      }
    }
  
  if(ncol(x) >= 9){
    x[5, c(1, 2)] <- 0
    x[6, c(3, 4)] <- 0
    x[7, c(2, 3, 4)] <- 0
    }
  return(x)
  
}

amat <- config(amat)


bmat <- replace(bmat, bmat == 1, NA)

```


### Structural Vector Autoregression
```{r}
model.1 <- SVAR(VAR(main_time[, -c(8:10)], p = 2, type = "none"), estmethod = "scoring", Amat = amat, Bmat = bmat, max.iter = 1000)
model.2 <- SVAR(VAR(main_time[, -c(11:13)], p = 2, type = "none"), estmethod = "scoring", Amat = amat, Bmat = bmat, max.iter = 1000)
```


```{r}
amat <- amat %>% as.data.frame()  %>% 
  rename(`Fed Rate` = V1, `Firm Capital` = V2, `PCE` = V3, `US Firm Profits` = V4,
         `Taxes` = V5, `Govt Ex` = V6, `US Public Debt` = V7, `1-Year` = V8, 
         `5-year` = V9, `10-year` = V10) 
amat[is.na(amat)] <- "."
rownames(amat) <- colnames(amat)

kbl(amat) %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  save_kable("ordering_svar_1.jpeg")


```


```{r}
#Computes the irfs and structures the 95% conf intervals into dataframes
#Names would be order based on the columns after making the dataframe
#So irf Govt Note and irf Govt ACMTP, with Govt being the impulse and ACMTP being response
compute <- function(model, impulse, response, steps){
  
    irfs <- irf(model, impulse = impulse, 
    response = response, n.ahead = steps, 
    runs = 1000, ci = 0.95)
    
    names <- c(response,
           paste0("lower_", response),
           paste0("upper_", response))
    
    naw <- as.data.frame(irfs[c(1:3)])
    colnames(naw) <- names
    
    return(naw)
}

#Helps graph the actual IRF
graph_irf <- function(data, steps){
  
  data$steps <- (0:steps)
  data <- data[, c(4, 1, 2, 3)]
  names <- colnames(data)
  
  colnames(data) <- c("w", "x", "y", "z") 
  
  return(ggplot(data, aes(w, x)) +
      geom_line(color = "black") +
      geom_line(aes(y = y), color = "red", linetype = "dashed") +
      geom_line(aes(y = z), color = "red", linetype = "dashed") +
      geom_hline(yintercept = 0, linetype = "solid", color = "red", size = 2) +
      labs(x = names[1], y = names[2])) 
  
}

# make it multiple functional
# include steps cause that column will not be included in compute section
staging <- function(model, impulse, response, steps){
    
    data <- compute(model, impulse, response, steps)
    
    m <- list()
    for(i in response){
      f <- data %>% select(contains(i))
      m <- append(m, list(f)) 
    }

  
  val <- lapply(m, graph_irf, steps)
  
  return(val)
  
}

#Takes a patchwork object and spits out a specific arrangement 
arrangement <- function(patches, deviate, react_var, setwd){
  setwd(setwd)
  
  patches <- patches + plot_annotation(title = paste0("Impulsing ", deviate, " on ", react_var), 
                                       theme = theme(plot.title = element_text(size = 14)) & 
                                       theme(text = element_text('serif')))
  
  ggsave(paste0(deviate, "_", react_var,"_irf.png"), patches)
  
}

setup <- function(x){
   
  if(length(x[[1]]) == 3){ 
    patches <- plot_spacer() + free(x[[1]][[1]], type = "space", side = "l") + x[[1]][[2]] + x[[1]][[3]]
    arrangement(patches, x[[2]], x[[3]], x[[4]])
  }
  else{
    patches <- (x[[1]][[1]] + grid::textGrob(' ')) / (x[[1]][[2]] + grid::textGrob(' '))
    
    arrangement(patches, x[[2]], x[[3]], x[[4]])
  }
  
}

#Stages the graphing sequence for the three yields 
#staging <- function(data, type, title, steps, file_name){
  
#  g1 <- as.data.frame(cbind(data %>% select(contains(type[[1]])), steps))
#  g2 <- as.data.frame(cbind(data %>% select(contains(type[[2]])), steps))
#  g3 <- as.data.frame(cbind(data %>% select(contains(type[[3]])), steps))
  
#  g1 <- graph_irf(g1, steps, DGS1, lower_DGS1, upper_DGS1)
#  g2 <- graph_irf(g2, steps, DGS5, lower_DGS5, upper_DGS5)
#  g3 <- graph_irf(g3, steps, DGSTEN, lower_DGSTEN, upper_DGSTEN)
  
#  patches <- plot_spacer() + free(g1, type = "space", side = "l") +
#      g2 + g3 
  
#  patches <- patches + plot_annotation(title = paste0("Impulsing ", title, " on Yields"), 
#                                      theme = theme(plot.title = element_text(size = 14)) & 
#                                       theme(text = element_text('serif')))
  
#  ggsave(paste0(file_name, "_irf.png"), patches)
 
#}




#Stages the graphing sequence for the three term premiums 
#staging_tps <- function(data, type, title, steps, file_name){
  
#  g1 <- as.data.frame(cbind(data %>% select(contains(type[[1]])), steps))
#  g2 <- as.data.frame(cbind(data %>% select(contains(type[[2]])), steps))
#  g3 <- as.data.frame(cbind(data %>% select(contains(type[[3]])), steps))
  
#  g1 <- graph_irf(g1, steps, ACMTP01, lower_ACMTP01, upper_ACMTP01)
#  g2 <- graph_irf(g2, steps, ACMTP05, lower_ACMTP05, upper_ACMTP05)
#  g3 <- graph_irf(g3, steps, ACMTP10, lower_ACMTP10, upper_ACMTP10)
  
#  patches <- plot_spacer() + free(g1, type = "space", side = "l") +
#      g2 + g3 
  
#  patches <- patches + plot_annotation(title = paste0("Impulsing ", title, " on Premiums"), 
#                                       theme = theme(plot.title = element_text(size = 14)) & 
#                                       theme(text = element_text('serif')))
  
  
#  ggsave(paste0(file_name, "_irf.png"), patches)
 
  
#}

#staging_sup <- function(data, type, title, steps, file_name){
  
#  g1 <- as.data.frame(cbind(data %>% select(contains(type[[1]])), steps))
#  g2 <- as.data.frame(cbind(data %>% select(contains(type[[1]])), steps))
  
#  g1 <- graph_irf(g1, steps, Bill)
#  g2 <- graph_irf(g2, steps, Note)
  
#  patches <- g1 + g2 
  
#  patches <- patches + plot_annotation(title = paste0("Impulsing ", title, " on Premiums"), 
#                                       theme = theme(plot.title = element_text(size = 14)) & 
#                                       theme(text = element_text('serif')))
  
#}

```



```{r}
response <- c("DGS1", "DGS5", "DGSTEN")

val <- staging(model.1, "Govt.Ex", response, steps = 25)
las <- staging(model.1, "Taxes", response, steps = 25)
stu <- staging(model.1, "US.Debt", response, steps = 25) 

graphs <- list(list(val, "Govt Ex", "Yields", "~/GitHub/2025_master_thesis/irfs_yields"), 
               list(las, "Taxes", "Yields", "~/GitHub/2025_master_thesis/irfs_yields"), 
               list(stu, "US Public Debt", "Yields", "~/GitHub/2025_master_thesis/irfs_yields"))

lapply(graphs, setup)
```


```{r}
response <- c("ACMTP01", "ACMTP05", "ACMTP10")

val <- staging(model.2, "Govt.Ex", response, steps = 25)
las <- staging(model.2, "Taxes", response, steps = 25)
stu <- staging(model.2, "US.Debt", response, steps = 25) 

graphs <- list(list(val, "Govt Ex", "Premiums", "~/GitHub/2025_master_thesis/irfs_premiums"), 
               list(las, "Taxes", "Premiums", "~/GitHub/2025_master_thesis/irfs_premiums"), 
               list(stu, "US Public Debt", "Premiums", "~/GitHub/2025_master_thesis/irfs_premiums"))

lapply(graphs, setup)
```


```{r}
sec_count <- read.csv("~/GitHub/2025_master_thesis/securities_count.csv")
sec_count <- sec_count %>%
  mutate(other_dates = as.Date(Record.Date)) %>%
  filter(Security.Type.Description == "Marketable",
         Security.Class.Description %in% c("Bill", "Bond", "Note"),
         other_dates > "2006-04-30") %>%
  pivot_wider(
    names_from = Security.Class.Description,
    values_from = Securities.Sold.Count) %>%
  arrange(other_dates) %>%
  mutate(dates = ceiling_date(as.Date(as.yearqtr(paste0(year(other_dates),"-Q",quarter(other_dates)), 
         format("%Y-Q%q"), frac = 0)),"quarter")) %>%
  select(c("dates", "Bill", "Note")) %>%
  group_by(dates) %>%
  summarize(
    Bill = sum(Bill),
    Note = sum(Note)
  )
  
sec_count[, -1] <- as.data.frame(apply(sec_count[, -1], 2, function(x) log(x) - lag(log(x))))
sec_count <- na.omit(sec_count)


m <- as.data.frame(t(sapply(sec_count[,c(2,3)], adf.test)))
kable(m[1, 4])
```


```{r}
other <- merge(main, sec_count)
other <- as.xts(other[, c(14, 27, 6, 29, 2, 20, 22, 38, 9, 39, 13)], order.by = other[, c(1)])
```


```{r}
amat <- bmat <- diag(1, 7, 7)
amat <- config(amat)

bmat <- replace(bmat, bmat == 1, NA)

```


```{r}

#This one includes Note supply and Term premiums for 10 year treasuries
model.3 <- SVAR(VAR(other[,-c(4, 7, 8, 9)], p = 2, type = "none"), estmethod = "scoring", Amat = amat, Bmat = bmat)

#This one includes Bill supply and Term premiums for 1 year treasuries
model.4 <- SVAR(VAR(other[,-c(4, 7, 10, 11)], p = 2, type = "none"), estmethod = "scoring", Amat = amat, Bmat = bmat) 

```


```{r}
response <- c("Note", "ACMTP10")

val <- staging(model.3, "Govt.Ex", response, steps = 10)
las <- staging(model.3, "Taxes", response, steps = 10)
stu <- staging(model.3, "Note", response, steps = 10) 

graphs <- list(list(val, "Govt Ex", "Note Supply and Premiums", "~/GitHub/2025_master_thesis/irfs_premiums"), 
               list(las, "Taxes", "Note Supply and Premiums", "~/GitHub/2025_master_thesis/irfs_premiums"), 
               list(stu, "T-Note Supply", "Note Supply and Premiums", "~/GitHub/2025_master_thesis/irfs_premiums"))
lapply(graphs, setup)

```




```{r}
response <- c("Bill", "ACMTP01")

val <- staging(model.4, "Govt.Ex", response, steps = 10)
las <- staging(model.4, "Taxes", response, steps = 10)
stu <- staging(model.4, "Bill", response, steps = 10) 

graphs <- list(list(val, "Govt Ex", "Bill Supply & Premiums", "~/GitHub/2025_master_thesis/irfs_premiums"), 
               list(las, "Taxes", "Bill Supply & Premiums", "~/GitHub/2025_master_thesis/irfs_premiums"), 
               list(stu, "Bill Supply", "Bill Supply & Premiums", "~/GitHub/2025_master_thesis/irfs_premiums"))
lapply(graphs, setup)

```




### Test of Cointegration

These are long run economic variables that are being examined through the vector autoregression. So, in accordance with that fact, I went
ahead conducted the Johansen test for cointegration. Cointegration can simply be looked at as when multiple series behaving the same way across
time
```{r}
summary(ca.jo(main_time, ecdet = "none", type = "trace", K = 2))
```




## Citations

Kilian, L., & Lütkepohl, H. (2018). Structural Vector Autoregressive Analysis Lutz Kilian, Helmut Lütkepohl. Cambridge University Press. 

